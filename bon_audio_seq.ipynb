{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries that we need\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import random\n",
    "import unicodedata\n",
    "import string\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# Defining the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the paths-traductions dataframe\n",
    "DF = pd.read_csv(\"transcript.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_token = 0\n",
    "SEP_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"CLS\", 1: \"SEP\"}\n",
    "        self.n_words = 2  # Count CLS and EOS\n",
    "        self.max_length = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        split = sentence.split(' ')\n",
    "        if self.max_length < len(split):\n",
    "            self.max_length = len(split)\n",
    "        for word in split:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = str(s)\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is done !\n",
      "CPU times: user 2.16 s, sys: 13.3 ms, total: 2.18 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Normalization\n",
    "output_lang = Lang(\"english\")\n",
    "\n",
    "DF[\"traductions\"] = DF[\"traductions\"].apply(lambda s: normalizeString(s))\n",
    "\n",
    "DF[\"traductions\"].apply(lambda s: output_lang.addSentence(s))\n",
    "\n",
    "print(\"It is done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(SEP_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2raw(path_wav, device, duration=6, srate=16000, send_wave=True):\n",
    "    \"\"\" \n",
    "    Opens an wav audio file with librosa.\n",
    "        \n",
    "    :param path_wav:\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    max_length = duration * srate\n",
    "    # Open the audio file\n",
    "    wave, srate = librosa.load(path_wav, duration=duration, mono=True, sr=srate)\n",
    "    # Padd if too short or slice if too long\n",
    "    \n",
    "    if wave.shape[0] < max_length:\n",
    "        pass\n",
    "        #wave = np.pad(wave, (0, max_length - wave.shape[0]),'constant', constant_values=0)\n",
    "    else:\n",
    "        wave = wave[:max_length]\n",
    "        \n",
    "    wave = wave.reshape(1, 1, wave.shape[0])\n",
    "    \n",
    "    if send_wave == True:\n",
    "        return torch.tensor(wave).to(device)\n",
    "    else:\n",
    "        return path_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06868c34db0a47d2823d676c116428e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 818 ms, sys: 17.3 ms, total: 835 ms\n",
      "Wall time: 850 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# La plus longue sÃ©quence\n",
    "values = DF['paths'][0:5].progress_apply(lambda path: wav2raw(path, device, duration=10, srate=16000, send_wave=False)).values\n",
    "max_path = np.argmax(values)\n",
    "max_val = np.max(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_path = DF[\"paths\"][max_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "spec = wav2raw(spec_path, device)\n",
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../english-bible/Genesis/Genesis_1-5.wav\n"
     ]
    }
   ],
   "source": [
    "print(spec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,  hidden_size=256):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_dir = 2\n",
    "        self.batch_size = 1 # For the moment    \n",
    "\n",
    "        self.conv1d1 = nn.Conv1d(1,  self.hidden_size, kernel_size=5)\n",
    "        self.conv1d2 = nn.Conv1d(self.hidden_size, self.hidden_size, kernel_size=5)\n",
    "        self.conv1d3 = nn.Conv1d(self.hidden_size, self.hidden_size, kernel_size=3)\n",
    "        self.conv1d4 = nn.Conv1d(self.hidden_size, self.hidden_size, kernel_size=3)\n",
    "        \n",
    "        self.pool1d1 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.pool1d2 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.pool1d3 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.pool1d4 = nn.MaxPool1d(kernel_size=3)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(0.3)\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.gru_1 = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=1, bidirectional=True)\n",
    "\n",
    "\n",
    "    def forward(self, wave, hidden):\n",
    "\n",
    "        # Started with the convolutionnal base\n",
    "        conv_layer = self.conv1d1(wave)\n",
    "        pool_layer = self.pool1d1(conv_layer)\n",
    "        \n",
    "        conv_layer = self.conv1d2(pool_layer)\n",
    "        pool_layer = self.pool1d2(conv_layer)\n",
    "        \n",
    "        conv_layer = self.conv1d3(pool_layer)\n",
    "        pool_layer = self.pool1d2(conv_layer)\n",
    "        \n",
    "        conv_layer = self.conv1d3(pool_layer)\n",
    "        pool_layer = self.pool1d3(conv_layer)\n",
    "\n",
    "        conv_layer = self.conv1d4(pool_layer)\n",
    "        pool_layer = self.pool1d4(conv_layer)\n",
    "        \n",
    "        dim_1 = pool_layer.shape[1]\n",
    "        dim_2 = pool_layer.shape[2]\n",
    "        \n",
    "        # Reshape from the conv (batch, features, seq_length) to (batch, seq_length, features) \n",
    "        output = pool_layer.reshape(1, dim_2, dim_1)\n",
    "        output, hidden = self.gru_1(output, hidden)\n",
    "        # Separate the forward pass\n",
    "        forward = output.view(1, dim_2, 2, self.hidden_size)[:, :, 0, :]\n",
    "        # Separate the backward pass\n",
    "        backward = output.view(1, dim_2, 2, self.hidden_size)[:, :, 1, :]\n",
    "        # Sum the forward pass and the backward to form the output\n",
    "        output = (forward + backward) / 2\n",
    "        output = F.relu(output)\n",
    "        # Pass through the dropout layer\n",
    "        output = self.dropout_2(output)\n",
    "        # I don't know what i will do with but i collect the last state for the moment\n",
    "        self.last_state = hidden.view(2, 1, self.hidden_size)[-1,:,:].unsqueeze(0).to(device)\n",
    "        return output.squeeze(0), hidden \n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([394, 256])\n"
     ]
    }
   ],
   "source": [
    "# Let test the encoder \n",
    "encoder = EncoderRNN().to(device)\n",
    "hidden = encoder.initHidden()\n",
    "encoder(spec, hidden)\n",
    "enc_out, enc_state = encoder(spec, hidden)\n",
    "print(enc_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1000#enc_out.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,  output_size, max_length, dropout_p=0.1,  hidden_size=256):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2 + self.max_length, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, attn_weights, encoder_outputs):\n",
    "        \n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # Compute the energie\n",
    "        cat = torch.cat((embedded[0], hidden[0], attn_weights), 1)\n",
    "        score = self.attn(cat)\n",
    "        # Compute the attention weights\n",
    "        attn_weights = F.softmax(score, dim=1)\n",
    "        #print(\"att weights\", attn_weights.shape)\n",
    "        # Apply the attention weights to the encoder outputs\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        # Combine it with the inputs to predict the next word\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # Compute the probabilities for the next word\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device), torch.zeros(1, self.max_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AttnDecoderRNN(output_lang.n_words, max_length=394, dropout_p=0.1).to(device)\n",
    "hidden, attn_w = decoder.initHidden()\n",
    "x = torch.tensor([0], device=device)\n",
    "dec_out, _, _ = decoder(x, hidden, attn_w, enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # Initialize the encoder and the decoder's attention weights\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    _, decoder_attention = decoder.initHidden()\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    target_length = target_tensor.size(0)\n",
    "    loss = 0\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    # We choose to encode all the raw audio directly (so here our approach is diffrent from NLP \n",
    "    # From Scratch tutorial)\n",
    "    encoder_outputs_of_input, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    # Impute the encoder_outputs with encoder_outputs_of_input\n",
    "    dim_0, dim_1 = encoder_outputs_of_input.shape\n",
    "    encoder_outputs[0 : dim_0 , 0 : dim_1] = encoder_outputs_of_input\n",
    "    \n",
    "\n",
    "    decoder_input = torch.tensor([[CLS_token]], device=device)\n",
    "    # We take only the last state of the encoder to initialize the decoder\n",
    "    decoder_hidden = encoder.last_state\n",
    "\n",
    "    use_teacher_forcing = True #if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_attention, encoder_outputs)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        pass\n",
    " \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    since = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "    \n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    training_pairs = DF.values.tolist()\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        # Shuffle the list at the beginning of an epoch \n",
    "        if iter % len(training_pairs) == 0:\n",
    "            random.shuffle(training_pairs)\n",
    "            \n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        path = training_pair[0]\n",
    "        input_tensor = wav2raw(path, device)\n",
    "        target_tensor = tensorFromSentence(output_lang, training_pair[1])\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == print_every - 1:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\"Iteration: {} \\t Evolution: {} % \\t Loss: {} \" .format(iter+1, round(iter / n_iters * 100, 1), round(print_loss_avg, 4)))\n",
    "            \n",
    "            \n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "        \n",
    "    print(\"Time taken for training: {}\".format(asMinutes(time.time() - since)))\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93249"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.shape[0] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200 \t Evolution: 0.2 % \t Loss: 5.934 \n",
      "Iteration: 400 \t Evolution: 0.4 % \t Loss: 5.536 \n",
      "Iteration: 600 \t Evolution: 0.6 % \t Loss: 5.1362 \n",
      "Iteration: 800 \t Evolution: 0.9 % \t Loss: 4.8368 \n",
      "Iteration: 1000 \t Evolution: 1.1 % \t Loss: 4.7133 \n",
      "Iteration: 1200 \t Evolution: 1.3 % \t Loss: 4.7312 \n",
      "Iteration: 1400 \t Evolution: 1.5 % \t Loss: 4.4528 \n",
      "Iteration: 1600 \t Evolution: 1.7 % \t Loss: 4.5729 \n",
      "Iteration: 1800 \t Evolution: 1.9 % \t Loss: 4.1571 \n",
      "Iteration: 2000 \t Evolution: 2.1 % \t Loss: 4.3228 \n",
      "Iteration: 2200 \t Evolution: 2.4 % \t Loss: 4.545 \n",
      "Iteration: 2400 \t Evolution: 2.6 % \t Loss: 4.3164 \n",
      "Iteration: 2600 \t Evolution: 2.8 % \t Loss: 4.1996 \n",
      "Iteration: 2800 \t Evolution: 3.0 % \t Loss: 3.3614 \n",
      "Iteration: 3000 \t Evolution: 3.2 % \t Loss: 3.2136 \n",
      "Iteration: 3200 \t Evolution: 3.4 % \t Loss: 3.8517 \n",
      "Iteration: 3400 \t Evolution: 3.6 % \t Loss: 3.7372 \n",
      "Iteration: 3600 \t Evolution: 3.9 % \t Loss: 3.8455 \n",
      "Iteration: 3800 \t Evolution: 4.1 % \t Loss: 2.9968 \n",
      "Iteration: 4000 \t Evolution: 4.3 % \t Loss: 2.9753 \n",
      "Iteration: 4200 \t Evolution: 4.5 % \t Loss: 4.0667 \n",
      "Iteration: 4400 \t Evolution: 4.7 % \t Loss: 4.0975 \n",
      "Iteration: 4600 \t Evolution: 4.9 % \t Loss: 3.7871 \n",
      "Iteration: 4800 \t Evolution: 5.1 % \t Loss: 3.4728 \n",
      "Iteration: 5000 \t Evolution: 5.4 % \t Loss: 4.0935 \n",
      "Iteration: 5200 \t Evolution: 5.6 % \t Loss: 4.001 \n",
      "Iteration: 5400 \t Evolution: 5.8 % \t Loss: 3.6941 \n",
      "Iteration: 5600 \t Evolution: 6.0 % \t Loss: 4.0224 \n",
      "Iteration: 5800 \t Evolution: 6.2 % \t Loss: 4.1138 \n",
      "Iteration: 6000 \t Evolution: 6.4 % \t Loss: 4.0976 \n",
      "Iteration: 6200 \t Evolution: 6.6 % \t Loss: 3.8541 \n",
      "Iteration: 6400 \t Evolution: 6.9 % \t Loss: 4.1387 \n",
      "Iteration: 6600 \t Evolution: 7.1 % \t Loss: 3.8882 \n",
      "Iteration: 6800 \t Evolution: 7.3 % \t Loss: 4.5608 \n",
      "Iteration: 7000 \t Evolution: 7.5 % \t Loss: 4.2248 \n",
      "Iteration: 7200 \t Evolution: 7.7 % \t Loss: 4.2451 \n",
      "Iteration: 7400 \t Evolution: 7.9 % \t Loss: 4.4435 \n",
      "Iteration: 7600 \t Evolution: 8.1 % \t Loss: 4.1626 \n",
      "Iteration: 7800 \t Evolution: 8.4 % \t Loss: 4.3237 \n",
      "Iteration: 8000 \t Evolution: 8.6 % \t Loss: 4.2161 \n",
      "Iteration: 8200 \t Evolution: 8.8 % \t Loss: 4.2746 \n",
      "Iteration: 8400 \t Evolution: 9.0 % \t Loss: 4.2646 \n",
      "Iteration: 8600 \t Evolution: 9.2 % \t Loss: 4.2615 \n",
      "Iteration: 8800 \t Evolution: 9.4 % \t Loss: 4.5786 \n",
      "Iteration: 9000 \t Evolution: 9.7 % \t Loss: 4.5611 \n",
      "Iteration: 9200 \t Evolution: 9.9 % \t Loss: 4.2324 \n",
      "Iteration: 9400 \t Evolution: 10.1 % \t Loss: 4.0304 \n",
      "Iteration: 9600 \t Evolution: 10.3 % \t Loss: 3.9957 \n",
      "Iteration: 9800 \t Evolution: 10.5 % \t Loss: 3.9927 \n",
      "Iteration: 10000 \t Evolution: 10.7 % \t Loss: 3.464 \n",
      "Iteration: 10200 \t Evolution: 10.9 % \t Loss: 3.9227 \n",
      "Iteration: 10400 \t Evolution: 11.2 % \t Loss: 4.6987 \n",
      "Iteration: 10600 \t Evolution: 11.4 % \t Loss: 4.156 \n",
      "Iteration: 10800 \t Evolution: 11.6 % \t Loss: 4.3869 \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "encoder1 = EncoderRNN().to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(output_lang.n_words, dropout_p=0.1, max_length=MAX_LENGTH).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 93249, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea make a max_length directly in the encoder so that we will give it automatically to the decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
